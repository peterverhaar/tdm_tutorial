{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d1dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from tdmh import *\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_distances , euclidean_distances\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bc1752",
   "metadata": {},
   "source": [
    "## Collect all the titles\n",
    "\n",
    "Firstly, we collect all the titles of the files in the corpus. The texts in the corpus are saved in a list named `corpus`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df13198b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "corpus = []\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if not(re.search(r'^\\.' , file)): \n",
    "        path = os.path.join(dir,file)\n",
    "        corpus.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da938c83",
   "metadata": {},
   "source": [
    "## Collect data about the texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b045e9",
   "metadata": {},
   "source": [
    "Next, we collect data about the number of tokens, the number of sentences, the type-token ratio, the sentiment score and the frequencies of specific POS categories. We collect these data for each text in the corpus. \n",
    "\n",
    "The code also adds values for categorical variables, copied from a file named `metadata.csv`. For more information, see the notebook named `Metadata.ipynb`. The categorical variables that need to be added must be specified in the list named `categorical_variables` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b81200b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('metadata.csv')\n",
    "metadata = metadata.set_index('title')\n",
    "categorical_variables = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f07f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(path):\n",
    "    title = os.path.basename(path)\n",
    "    title = re.sub( r'[.]txt$' , '' , title )\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d729b11e",
   "metadata": {},
   "source": [
    "This notebook can collect data both for text in English and and for text in Dutch. If the texts in the corpus are in Dutch, the value of the variable `language` needs to be changed in the cell below.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b595a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttr_cap = 3000\n",
    "\n",
    "language = 'eng'\n",
    "#language = 'dut'\n",
    "\n",
    "\n",
    "if language == 'eng':\n",
    "    from nltk import pos_tag\n",
    "elif language == 'dut':\n",
    "    import nltk\n",
    "    nltk.download('alpino')\n",
    "    from nltk.corpus import alpino as alp\n",
    "    from nltk.tag import UnigramTagger, BigramTagger\n",
    "    training_corpus = alp.tagged_sents()\n",
    "    unitagger = UnigramTagger(training_corpus)\n",
    "    bitagger = BigramTagger(training_corpus, backoff=unitagger)\n",
    "    pos_tag = bitagger.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f2fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = open( 'data.csv' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "## Sentiment Analysis\n",
    "ana = SentimentIntensityAnalyzer()\n",
    "\n",
    "if language == 'eng':\n",
    "    pos_tags = ['JJ','JJR','JJS','VBD','MD','RB','RBR','RBS']\n",
    "elif language == 'dut':\n",
    "    pos_tags = ['verb','adv','adj','comp','noun', 'prep']\n",
    "    \n",
    "\n",
    "## Header of the CSV file\n",
    "out.write('title,tokens,sentences,ttr,sentiment')\n",
    "\n",
    "for t in pos_tags:\n",
    "    out.write(f',{t}')\n",
    "\n",
    "for c in categorical_variables:\n",
    "    out.write(f',{c}')\n",
    "\n",
    "out.write('\\n')\n",
    "\n",
    "for text in corpus:\n",
    "    \n",
    "    # The dictionary named 'data' saves the data about the text\n",
    "    data = dict()\n",
    "    print( f'Analysing {text} ...')\n",
    "    \n",
    "    ## Get the title, based on the filename\n",
    "    title = extract_title( text )\n",
    "    \n",
    "    ## read the full text\n",
    "    fh = open( text, encoding = 'utf-8')\n",
    "    full_text = fh.read()\n",
    "    \n",
    "    ## count the number of sentences\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    data['nr_sentences'] = len(sentences)\n",
    "    \n",
    "    # dictionary to count the POS tags\n",
    "    freq_pos = dict()    \n",
    "    \n",
    "    # variables for the calculation of type-token ratio\n",
    "    \n",
    "    freq_ttr = dict()\n",
    " \n",
    "    # token count is initalised at 0\n",
    "    data['nr_tokens'] = 0\n",
    "    \n",
    "    # list for the sentiment scores\n",
    "    all_scores = []\n",
    "\n",
    "        \n",
    "    for s in sentences:\n",
    "        words = word_tokenize(s)\n",
    "        words = remove_punctuation(words)\n",
    "        \n",
    "        tags = pos_tag(words)\n",
    "        \n",
    "        scores = ana.polarity_scores(s)\n",
    "        all_scores.append( scores[\"compound\"] )\n",
    "        \n",
    "        # Each tag consists of two values: \n",
    "        # [0]: the word and [1] the POS tag\n",
    "        for word_tag in tags:\n",
    "            word = word_tag[0]\n",
    "            tag = word_tag[1]\n",
    "            \n",
    "            # count the tokens\n",
    "            data['nr_tokens'] += 1\n",
    "            \n",
    "            # place tokens in dictionary freq_ttr\n",
    "            # only if the word count is less than ttr_cap\n",
    "            # The nr of items in the dictionary eventually equals the nr of types\n",
    "            if data['nr_tokens'] <= ttr_cap:\n",
    "                freq_ttr[ word ] = freq_ttr.get( word , 0 ) + 1\n",
    "                \n",
    "            ## Count frequencies of all the POS tags\n",
    "            freq_pos[ tag ] = freq_pos.get( tag ,0) +1\n",
    "            \n",
    "    for t in pos_tags:\n",
    "        data[t] = freq_pos.get(t,0)\n",
    "    data['sentiment'] = sum(all_scores) / len(all_scores)\n",
    "\n",
    "                \n",
    "    # Calculate TTR: number of items in freq_ttr dictyionary\n",
    "    # divided by ttr_cap\n",
    "    data['ttr'] = len( freq_ttr ) / ttr_cap\n",
    "    \n",
    "    # write the results to a CSV file    \n",
    "    out.write( f\"{title},{data['nr_tokens']},{data['nr_sentences']},{data['ttr']},{data['sentiment']}\" )\n",
    "    for t in pos_tags:\n",
    "        out.write( f\",{data[t] }\"  )\n",
    "        \n",
    "    for c in categorical_variables:\n",
    "        out.write( f\",{metadata.loc[title][c] }\"  )\n",
    "        \n",
    "    out.write('\\n')\n",
    "\n",
    "    \n",
    "out.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4514f8",
   "metadata": {},
   "source": [
    "## Read the data from the CSV\n",
    "\n",
    "Once the CSV file containing all the data has been created, we can start the analyses. The data is firstly read using the `read_csv()` method from `pandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c313e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4074d816",
   "metadata": {},
   "source": [
    "## Visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69860601",
   "metadata": {},
   "source": [
    "In many of the visualisations below, the colours represent the different values for a categorical variable in the CSV file named 'metdata.csv'. \n",
    "In the cell below, specify the column containing the values that need to be visualised. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db03459",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_variable = 'Class'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55093d4",
   "metadata": {},
   "source": [
    "### Average sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.sort_values(by=categorical_variable)\n",
    "\n",
    "x_axis = data['title']\n",
    "y_axis = data['tokens'] / data['sentences']\n",
    "\n",
    "\n",
    "hue = data[categorical_variable]\n",
    "\n",
    "fig = plt.figure( figsize=( 6 , 5 ) )\n",
    "\n",
    "\n",
    "graph = sns.barplot( x=x_axis, y=y_axis, hue=hue , dodge=False )\n",
    "\n",
    "#graph.set_title('' , size = 20) \n",
    "graph.set_xlabel('Novel' , size = 14) \n",
    "graph.set_ylabel('Average sentence length' , size = 14 )\n",
    "\n",
    "plt.xticks(rotation= 80)\n",
    "# The next line places the legend outside out the plot\n",
    "plt.legend( bbox_to_anchor=(1.05, 1),loc=2, borderaxespad=0.);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4728e4",
   "metadata": {},
   "source": [
    "### Type-token ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5ab29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = data['title']\n",
    "y_axis = data['ttr']\n",
    "hue = data[categorical_variable]\n",
    "\n",
    "data = data.sort_values(by= categorical_variable )\n",
    "\n",
    "fig = plt.figure( figsize=( 6 , 5 ) )\n",
    "\n",
    "#df_sorted = df.sort_values(by=[ y_axis] , ascending = False)\n",
    "\n",
    "graph = sns.barplot( x=x_axis, y=y_axis, hue=hue , dodge=False )\n",
    "\n",
    "#graph.set_title('' , size = 20) \n",
    "graph.set_xlabel('Novel' , size = 14) \n",
    "graph.set_ylabel('Type-token ratio' , size = 14 )\n",
    "\n",
    "plt.xticks(rotation= 80)\n",
    "# The next line places the legend outside out the plot\n",
    "plt.legend( bbox_to_anchor=(1.05, 1),loc=2, borderaxespad=0.);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5c995",
   "metadata": {},
   "source": [
    "### Modal verbs\n",
    "\n",
    "Can you add code to do a comparative analysis of the use of modal verbs? The POS code for modal verbs is 'MD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40202c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdf3f990",
   "metadata": {},
   "source": [
    "### Adjectives and Adverbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b557c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['adjectives'] = data['JJ'] + data['JJS'] + data['JJS']\n",
    "data['adverbs'] = data['RB'] + data['RBR'] + data['RBS'] \n",
    "data['adverbs'] = data['VBD'] \n",
    "data['sentence_length'] = data['tokens'] / data['sentences']\n",
    "\n",
    "# Columns to use in the visualisation\n",
    "y_axis = 'adjectives'\n",
    "x_axis = 'adverbs'\n",
    "point_size =  'sentence_length'\n",
    "point_colour = categorical_variable\n",
    "\n",
    "#colours = [ '#fab31b' , '#3c2ebf'  ] \n",
    "nr_options = len(data[categorical_variable].unique())\n",
    "colours = list(sns.color_palette(\"colorblind\", nr_options ))\n",
    "\n",
    "fig = plt.figure( figsize = ( 10,10 ))\n",
    "\n",
    "## This line adds spacing in between the lines of the legend \n",
    "sns.set(rc = {'legend.labelspacing': 1.6})\n",
    "\n",
    "ax = sns.scatterplot( data=data , x=x_axis, y=y_axis,  \n",
    "                     hue= point_colour, size= point_size, sizes=( 100 , 1000) , \n",
    "                      palette = colours  )\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    plt.text( row[x_axis], row[y_axis] , row['title'] , fontsize=12.8)\n",
    "\n",
    "\n",
    "ax.set_xlabel( x_axis  , fontsize = 20 )\n",
    "ax.set_ylabel( y_axis  , fontsize = 20 )\n",
    "# ax.set_title( '' , fontsize=24 )\n",
    "\n",
    "\n",
    "# # this next line places the legend outside of the graph\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);\n",
    "\n",
    "# plt.savefig( 'scatterplot.png' , dpi=300 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb16153",
   "metadata": {},
   "source": [
    "## Type-token ratio and average sentence length\n",
    "\n",
    "Can you create a scatter plot which can be used to explore the values for the type-token ratio and average sentence length? Also provide suitable labels for the X-axis and the Y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4cbf12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cd81f3b",
   "metadata": {},
   "source": [
    "# Diction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8ca84",
   "metadata": {},
   "source": [
    "To study the vocabulary, it can be useful to create a document-term matrix as a first step. \n",
    "In such a document-term matrix, the rows represent the documents in the collection and columns correspond to the types (i.e. the unique words that are found).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085e7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequencies = dict()\n",
    "all_words = Counter()\n",
    "all_lengths = dict()\n",
    "titles = []\n",
    "\n",
    "for book in corpus:\n",
    "    print(book)\n",
    "    row = []\n",
    "    \n",
    "    ## Extract the title from the filename\n",
    "    title = extract_title(book)\n",
    "    titles.append(title)\n",
    "    \n",
    "    ## Read the full text\n",
    "    file = open(book,encoding='utf-8')\n",
    "    full_text = file.read()\n",
    "    \n",
    "    ## Find all the words in the text\n",
    "    words = tokenise_remove_stopwords(full_text.lower())\n",
    "    all_lengths[title] = len(words)\n",
    "    \n",
    "    ## Count all the words in the text\n",
    "    freq_book = Counter(words)\n",
    "    all_frequencies[title] = freq_book\n",
    "\n",
    "    ## Update the Counter object named 'all_words'\n",
    "    all_words.update(words)\n",
    "        \n",
    "print('Done!')\n",
    "\n",
    "vocabulary = list(all_words.keys())\n",
    "\n",
    "dtm = pd.DataFrame( columns= all_words )\n",
    "        \n",
    "for book in titles:\n",
    "    row = dict()\n",
    "    freq = all_frequencies[book]\n",
    "    for w in vocabulary:\n",
    "        row[w] = freq.get(w,0)/all_lengths[book]\n",
    "    dtm = dtm.append( row  , ignore_index=True )\n",
    "    \n",
    "\n",
    "dtm.index = titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a5eeaf",
   "metadata": {},
   "source": [
    "## Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0258d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = dtm.transpose()\n",
    "\n",
    "def find_unique_words(tdm,book,nr_words=50):\n",
    "    columns_other = list(tdm.columns)\n",
    "    columns_other.remove(book)\n",
    "    tdm['sum_other'] = tdm[columns_other].sum(axis=1) \n",
    "    tdm = tdm.query( 'sum_other == 0' )\n",
    "    tdm = tdm.query( f'{book} > 0' )\n",
    "    tdm = tdm.sort_values(book,ascending=False)[book][:nr_words]\n",
    "    return list(tdm.keys())\n",
    "\n",
    "for file in corpus:\n",
    "    title = extract_title(file)\n",
    "    unique = tdm.copy()\n",
    "    unique_words = find_unique_words(unique,title,25)\n",
    "    print(f'\\n{title.upper()}\\n')\n",
    "    for word in unique_words:\n",
    "        print(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a3f49",
   "metadata": {},
   "source": [
    "## Distinctive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctive = tdm.copy()\n",
    "distinctive['average'] = distinctive.mean(numeric_only=True,axis=1)\n",
    "\n",
    "\n",
    "for book in titles:\n",
    "    column_name = f'{book}_distinction'\n",
    "    values = []\n",
    "    for word,freq in distinctive.iterrows():\n",
    "\n",
    "        score = (freq[book] - freq['average'])\n",
    "        if pd.isna(score):\n",
    "            score = 0\n",
    "        values.append(score)\n",
    "    distinctive[column_name] = values\n",
    "    \n",
    "def most_distinctive_words(df,book): \n",
    "    column_name = f'{book}_distinction'\n",
    "    return df[column_name].sort_values( ascending = False )[:30]\n",
    "\n",
    "for title in titles:\n",
    "    print(f'\\n{title.upper()}\\n')\n",
    "    for word,count in most_distinctive_words(distinctive,title).items():\n",
    "        print( f'{word} ({count}) ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e864791",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e212b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = all_words.most_common(20000)\n",
    "most_common_list = [word for word,count in most_common]\n",
    "dtm_mc = dtm[ most_common_list ]\n",
    "\n",
    "matrix = euclidean_distances(dtm_mc)\n",
    "matrix_df = pd.DataFrame( matrix , columns = titles , index = titles )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d507e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Heatmap\n",
    "ax = sns.heatmap( matrix_df , cmap=\"YlGnBu\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb2cc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "from scipy.cluster.hierarchy import ClusterWarning\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", ClusterWarning)\n",
    "\n",
    "\n",
    "linkages = linkage(matrix_df,'ward')\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "dendrogram( linkages , labels = matrix_df.index , orientation=\"right\", leaf_font_size=12, leaf_rotation=20)\n",
    "plt.tick_params(axis='x', which='both', bottom=False,\n",
    "top=False, labelbottom=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98feb421",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_dist = 0\n",
    "for row in matrix_df.mean():\n",
    "    mean_dist += row\n",
    "    \n",
    "mean_dist = mean_dist / len(matrix_df.mean())\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "nodes = [] \n",
    "edges = []\n",
    "\n",
    "\n",
    "related_texts = list(matrix_df.columns)\n",
    "\n",
    "## an edge is drawn in between two nodes\n",
    "# if the cosine similarity is 0.7 or higher\n",
    "min_similarity = 0.7\n",
    "\n",
    "for text,values in matrix_df.iterrows():\n",
    "    for rt in related_texts:\n",
    "        if text != rt:\n",
    "            if values[rt] < mean_dist:\n",
    "                edges.append( (text,rt) )\n",
    "                nodes.append(text)\n",
    "\n",
    "nodes = list( set(nodes) )\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "#pos = nx.gaussian_random_partition_graph(G)\n",
    "pos = nx.fruchterman_reingold_layout(G,scale=1)\n",
    "pos=nx.spring_layout(G,scale=0.5) \n",
    "\n",
    "G.add_nodes_from( nodes )\n",
    "G.add_edges_from( edges )\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "nx.draw(G , node_size=160 , with_labels = True , node_color='#6f12a1')\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa02f157",
   "metadata": {},
   "source": [
    "# Lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "\n",
    "baseUrl = 'https://raw.githubusercontent.com/peterverhaar/semanic-tagging/main/Lexicons/'\n",
    "lexicon_files = [  'Academic.txt' , 'Economics.txt' ,  'Legal.txt' , 'Military.txt' , 'Movement.txt' , 'Pain.txt' , 'Passive.txt' , 'Pleasure.txt' , 'Politics.txt' , 'Power.txt' , 'Religion.txt' , 'Space.txt' , 'Time.txt' , 'Transportation.txt' , 'Vice.txt' , 'Weather.txt' , 'workAndEmployment.txt' ]\n",
    "\n",
    "lexicons_dir = 'Lexicons'\n",
    "if not os.path.isdir(lexicons_dir):\n",
    "    os.mkdir(lexicons_dir)\n",
    "\n",
    "\n",
    "for l in lexicon_files:\n",
    "    topic = l[ : l.rindex('.') ]\n",
    "    response = requests.get( baseUrl + l)\n",
    "    words = []\n",
    "    if response:\n",
    "        response.encoding = 'utf-8'\n",
    "        out = open( os.path.join( lexicons_dir , l ) , 'w' , encoding = 'utf-8' )\n",
    "        out.write( response.text )\n",
    "        out.close()\n",
    "\n",
    "print('Lexicons have been downloaded!')\n",
    "\n",
    "\n",
    "import os \n",
    "from os.path import join\n",
    "import re\n",
    "\n",
    "lexicons = dict()\n",
    "\n",
    "lexicon_dir = 'Lexicons'\n",
    "\n",
    "for file in os.listdir(lexicon_dir):\n",
    "\n",
    "    topic = file[ : file.rindex('.') ]\n",
    "    words = []\n",
    "    \n",
    "    with open( join(lexicons_dir,file) , encoding = 'utf-8' ) as file_handler:   \n",
    "        for l in file_handler: \n",
    "            if re.search( r'\\w' , l ):\n",
    "                words.append(l.strip())\n",
    "\n",
    "    lexicons[topic] = words    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3481e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from tdmh import *\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "csv = open( 'lexicon.csv' , 'w' , encoding = 'utf-8' )\n",
    "\n",
    "## print header\n",
    "csv.write( 'title' )\n",
    "for l in lexicons:\n",
    "    csv.write( f',{l.lower().strip() }' )\n",
    "csv.write('\\n')\n",
    "\n",
    "\n",
    "\n",
    "for file in os.listdir( dir ):\n",
    "    if re.search( r'\\.txt$' , file ):\n",
    "        csv.write( remove_extension( file ) )\n",
    "        path = join( dir, file )\n",
    "        \n",
    "        print( '\\nLemmatising {} ...'.format( file ) )\n",
    "        with open( path , encoding = 'utf-8' ) as fh:\n",
    "            full_text = fh.read()\n",
    "        lemmatised = lemmatise(full_text)\n",
    "        \n",
    "        print( 'Performing semantic tagging for {} ...'.format( file ) )\n",
    "        \n",
    "        words = word_tokenize(lemmatised)\n",
    "        words = remove_punctuation(words)\n",
    "        freq = dict()\n",
    "        for w in words:\n",
    "            freq[w] = freq.get(w,0)+1\n",
    "        tokens = len(lemmatised)\n",
    "        \n",
    "        for l in lexicons:\n",
    "            print(f'{l} ...')    \n",
    "            \n",
    "            countOccurrences = 0\n",
    "            for word in l:\n",
    "                countOccurrences += freq.get(word,0)\n",
    "            \n",
    "            csv.write( ',{}'.format( countOccurrences / tokens ) )\n",
    "        csv.write('\\n')\n",
    "        \n",
    "csv.close()\n",
    "\n",
    "print(\"Done!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed137dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f96ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "x_axis = 'title'\n",
    "y_axis = 'religion'\n",
    "hue = data[categorical_variable]\n",
    "\n",
    "data = data.sort_values(by= categorical_variable )\n",
    "\n",
    "fig = plt.figure( figsize=( 6 , 5 ) )\n",
    "\n",
    "#df_sorted = df.sort_values(by=[ y_axis] , ascending = False)\n",
    "\n",
    "graph = sns.barplot( data = df , x=x_axis, y=y_axis, hue=hue , dodge=False )\n",
    "\n",
    "graph.set_title( y_axis.title() , size = 20) \n",
    "graph.set_xlabel('Text' , size = 14) \n",
    "graph.set_ylabel('Frequencies' , size = 14 )\n",
    "\n",
    "plt.xticks(rotation= 80)\n",
    "# The next line places the legend outside out the plot\n",
    "plt.legend( bbox_to_anchor=(1.05, 1),loc=2, borderaxespad=0.);\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5cc3ca9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
