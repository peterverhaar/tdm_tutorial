{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "\n",
    "A named entity is an object, person, location or organization which has been assigned a proper name. *Named Entity Recognition* is a computational technique that seeks to identify all the named entities that that are mentioned within texts. Applications making use of Named Entity Recognition can generally extract the most of the occurrences of such named entities and it can also characterise such entities using pre-defined categories such as ‘Person’, ‘Location’, ‘Work of Art’, ‘Organisation’. \n",
    "\n",
    "Named Entity Recognition applications typically make use of statistical models created using Machine Learning algorithms. Such models are often trained using large numbers of texts in which all the people, locations, organisations and named objects have been labelled manually by human readers. On the basis of a careful analysis of the frequencies and the contexts of all of these named entitities, computers can eventually be enabled to recognise similar types of entitities in new, unlabelled texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanza\n",
    "\n",
    "One of the tools that can be used for NER is [Stanza](https://stanfordnlp.github.io/stanza/). Stanza is based on the `Stanford CoreNLP` application. `Stanza` can be installed via `pip`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !pip install stanza\n",
    "# !pip install tqdm   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the package has been installed successfully, you can import the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to recognise named entities, you firstly need to create a `Pipeline` object. The `lang` parameter specifies the language model you want to work with. 'en' stands for English. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more models in [the documentation of the stanza package](https://stanfordnlp.github.io/stanza/available_models.html). The language code for 'Dutch' is 'nl'.\n",
    "\n",
    "The `Pipeline` object, which was named 'nlp' in the code above, can be used to analyse the named entities in strings. This text can be supplie directly as a parameter of the `nlp` object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = '''James Joyce (born February 2, 1882, Dublin, Ireland - died January 13, 1941,\n",
    "    Zürich, Switzerland) was Irish novelist noted for his experimental use of language and \n",
    "    exploration of new literary methods in such large works of fiction as Ulysses (1922)\n",
    "    and Finnegans Wake (1939).'''\n",
    "\n",
    "doc = nlp(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of this call is assigned to a variable named `doc`. This variable has a property named `ents` which lists all the named entities that were found in the string. Each named entity has a `text` and a `type` property. \n",
    "\n",
    "You can navigate across all of the named entities in a `for` loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ne in doc.ents:\n",
    "    print( f'{ne.text} (type: {ne.type})' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model for the English language was trained on the basis of an large annotated corpus named *[OntoNotes](https://catalog.ldc.upenn.edu/LDC2013T19)*. This model can assign named entities in the following categories:\n",
    "\n",
    "* PERSON: People, including fictional \n",
    "* NORP: Nationalities or religious or political groups \n",
    "* FAC: Buildings, airports, highways, bridges, etc. \n",
    "* ORG: Companies, agencies, institutions, etc. \n",
    "* GPE: Countries, cities, states \n",
    "* LOC: Non-GPE locations, mountain ranges, bodies of water \n",
    "* PRODUCT: Objects, vehicles, foods, etc. (not services) \n",
    "* EVENT: Named hurricanes, battles, wars, sports events, etc. \n",
    "* WORK_OF_ART: Titles of books, songs, etc. \n",
    "* LAW: Named documents made into laws. \n",
    "* LANGUAGE: Any named language \n",
    "* DATE: Absolute or relative dates or periods \n",
    "* TIME: Times smaller than a day \n",
    "* PERCENT: Percentage, including \"%\" \n",
    "* MONEY: Monetary values, including unit \n",
    "* QUANTITY: Measurements, as of weight or distance \n",
    "* ORDINAL: \"first\", \"second\", etc. \n",
    "* CARDINAL: Numerals that do not fall under another type \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding NER tags in longer texts\n",
    "\n",
    "One important limitation of the *Stanza* tagger is that it can only be applied to texts consisting of less than 1,000,000 characters. The parser roughly requires 1GB of memory per 100,000 characters, and texts containing more than 1,000,000 characters tends to cause memory allocation errors. \n",
    "\n",
    "The code below tries to avoid such errors. It safely sets the `max_length` of the texts to be parsed to 5000. The code divides the full text into segments, each of which are shorter than this `max_length`.  \n",
    "\n",
    "After this, these shorter segments are all parsed one by one. These tagged texts are stored in a dictionary named `tagged_segments`. \n",
    "\n",
    "Tagging texts of ca. 5000 characters still demands quite some memory space. The code below may take some time to complete because of this.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "segments = dict()\n",
    "tagged_segments = dict()\n",
    "segment_nr = 0\n",
    "\n",
    "text = 'ARoomWithAView.txt'\n",
    "dir = 'Corpus'\n",
    "path = join( dir, text )\n",
    "max_length = 5000\n",
    "\n",
    "with open(path, encoding = 'utf-8') as file_handler:\n",
    "    full_text = file_handler.read()\n",
    "    \n",
    "print( f'Total number of characters in {text}: {len(full_text)}' )\n",
    "\n",
    "sentences = sent_tokenize(full_text)\n",
    "\n",
    "length = 0 \n",
    "segment = ''\n",
    "\n",
    "for s in sentences:\n",
    "    length += len(s)\n",
    "    if length < max_length:\n",
    "        segment += s + ' '\n",
    "    else:\n",
    "        segments[segment_nr] = segment\n",
    "        segment = s + ' '\n",
    "        length = 0 \n",
    "        segment_nr += 1\n",
    "        \n",
    "if len(segment) > 0:\n",
    "    segments[segment_nr] = segment\n",
    "    \n",
    "print(f'{len(segments)} segments were created.')\n",
    "print( 'Annotating the text segments ... ')    \n",
    "for i in tqdm(segments,total = len(segments),desc=\"Progress: \"):\n",
    "    tagged_segments[i] = nlp(segments[i])\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The annotated texts can be analysed in a variety of ways. The code below lists the personal names that are mentioned most frequently in the text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmh import sortedByValue\n",
    "from collections import Counter\n",
    "\n",
    "names = []\n",
    "freq = Counter()\n",
    "\n",
    "\n",
    "for doc in tagged_segments:\n",
    "    for named_entity in tagged_segments[doc].ents:\n",
    "        if named_entity.type == 'NORP':\n",
    "            name = named_entity.text\n",
    "            #name = name.strip()\n",
    "            freq.update([name])\n",
    "\n",
    "for ne,count in freq.most_common(20):\n",
    "    print(ne,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "\n",
    "The NLP library named *spaCy* is a second tools that you can work with the analyse named entities. For more information on how to install *spaCy*, or on how to load specific language models, please read the notebook on *NLP*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*spaCy* offers support for a wide range of languages. As is the case for `stanza`, the model for the English language was trained on the basis of an large annotated corpus named *[OntoNotes](https://catalog.ldc.upenn.edu/LDC2013T19)*. This model can be downloaded using the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been downloaded, it needs to be  loaded, so that you can work with it in your code. The `load()` method in `spaCy` creates a new object which can be used to add linguistic and semantic annotations. Ii the code below, is object is given the name `nlp`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `nlp` object can annotate a given string in a number of ways. SpaCy can be used not only to describe properties such as the parts of speech and the lemmatised versions of words, but also find the named entities. \n",
    "In the code below, the output of the `nlp()` method is assigned to a variable named `tagged_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_text = nlp(\"James Joyce (born February 2, 1882, Dublin, Ireland - died January 13, 1941, Zürich, Switzerland) was Irish novelist noted for his experimental use of language and exploration of new literary methods in such large works of fiction as Ulysses (1922) and Finnegans Wake (1939).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tags added by `nlp()` can be visualised effectively using the `render()` method from the `displacy` module. When you add the parameter `style`, with value `ent`, this visulation concentrates on the named entities that have been found. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(tagged_text, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning of specifc *spaCy* codes can be found the `explain()` method, as is demonstated in the following code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['PERSON','NORP','FAC','ORG','GPE','LOC','PRODUCT','EVENT','WORK_OF_ART','LAW','LANGUAGE','DATE','TIME','PERCENT','MONEY','QUANTITY','ORDINAL','CARDINAL']\n",
    "\n",
    "for t in tags: \n",
    "    print( f'{t}: {spacy.explain(t)} ' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
