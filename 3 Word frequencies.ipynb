{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook on *Tokenisation* explained how we can divide a text into the separate words. Once we have managed to do this, we can begin to count the **types** (i.e. the occurrences of the unique words). \n",
    "\n",
    "We can use a dictionary for this specific task. This specific data structure is useful in this situation, as it can be used to store pairs of values. In a dictionary created to store word frequencies, we can use the types that found in the text as keys, and, as associated values, we can capture the number of times these types occur. \n",
    "\n",
    "To count the number of times of word occurs in a text, we can choose the following approach: \n",
    "\n",
    "1. Create a Python list of all the individual words (or *tokens*) of the text. \n",
    "2. Iterate over all of these words.\n",
    "3. Add each word as a key to a dictionary. At the first occurrence of the word, assign the value 1. For all the occurrences that follow, increment the current value associated with word by 1. \n",
    "\n",
    "The code below contains an implementation of these steps. It combines some of the concepts that have been discussed fo far. \n",
    "\n",
    "It firstly defined a string named `quote`. The individual words in this string are found using the `word_tokenize()` method. The `remove_punctuation()` method, defined in `tdmh`, removes tokes that do not contain any alphanumeric characters. \n",
    "\n",
    "The code then navigates across all the of the words in the string, using a `for` loop. The code is also assumed that the tokens should be processed in a case-insensitive ways. The token 'We' (starting with a capital) should be analysed as an instance of the token 'we' (with the first character in lower case). We can accomplish this by converted all words into lower case using the `lower()` method.\n",
    "\n",
    "The words that are found in this way will become keys in the dictionary named `freq` (short for 'frequency'). Initially, this dictionary is completely empty. At the first occurrence of a given word, the word will clearly not be in this dictionary yet. In this case, the `get()` method returns the value `0`. This initial value will immediately be incremented with value `1`. After this, the existing value will be updated, with every new occurrence of the word in the text. \n",
    "\n",
    "The program does this for each individual word. Once Python has processed the full text, the dictionary `freq` will have data about the occurrences of all the words in this full string. \n",
    "\n",
    "The final few lines of this code snippet print the frequencies of the all the words that were found in the quote.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from tdmh import * \n",
    "\n",
    "\n",
    "freq = dict()\n",
    "\n",
    "quote = '''It was the best of times, it was the worst of times, \n",
    "it was the age of wisdom, it was the age of foolishness, \n",
    "it was the epoch of belief, it was the epoch of incredulity'''\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "for w in words:\n",
    "    w = w.lower()\n",
    "    freq[w] = freq.get( w, 0 ) + 1\n",
    "         \n",
    "for f in freq:\n",
    "    word = 'occurrence'\n",
    "    if freq[f] > 1:\n",
    "        word += 's'\n",
    "    print( f' \"{f}\" => {freq[f]} {word}' )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys in the dictionary named `freq` are stored in a specific order: the order in which they were found in the original text fragment. \n",
    "\n",
    "When you aim to analyse word frequencies, you may want to display the items in a different order. It can be useful, for example, to sort the list by frequency, and to make sure that the words with the highest frequencies are shown at the top of the list. \n",
    "\n",
    "The standard `sorted()` function can only sort dictionaries on the basis of the keys, unfortunately. There is no standard method in Python that you can use to sort a dictionary by its values. To address this lacuna, we can define a new method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sortedByValue( dict , ascending = True ):\n",
    "    if ascending: \n",
    "        return {k: v for k, v in sorted(dict.items(), key=lambda item: item[1])}\n",
    "    else:\n",
    "        return {k: v for k, v in reversed( sorted(dict.items(), key=lambda item: item[1]))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this tutorial, you do not need to understand the full details of this new method. For the time being, you can treat this function as a 'black box'. This function demands a dictionary as input, and it returns a **sorted** dictionary as output. importantly, the function sorts by the **values** and not by the keys. \n",
    "\n",
    "The cell below offers a demonstration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sortedByValue( freq ) :\n",
    "    print( f'{f} -> {freq[f]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `sortedByValue()` sorts the values in an ascending order by default. To display the values in a descending order, you can supply a parameter named `ascending` with value `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in sortedByValue( freq , ascending = False ) :\n",
    "    print( f'{f} -> {freq[f]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "In normal texts, lexical categories such pronouns, prepositions or articles (words such as 'the', 'of' and 'it') are generally most common. Such words are sometimes referred to as 'function words'. Words such as these are important for producting grammatically correct sentences, but they mostly have little expessive value independently.\n",
    "\n",
    "If you are interested in studying the actual contents or the semantics of a text, it can be useful to remove such frequently used function words. \n",
    "\n",
    "If you want to do this, one option is to make use of the list of stopwords available as part of the `nltk` package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is is a revision of the code that was discussed earlier for calculating the word frequencies. The only difference is that there is a condition in the first `for` loop. This condition has the effect that words will be added to the dictionary `freq` only if they are NOT on the list of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = dict()\n",
    "\n",
    "words = word_tokenize(quote)\n",
    "words = remove_punctuation(words)\n",
    "\n",
    "\n",
    "for w in words:\n",
    "    w = w.lower()\n",
    "    if w not in stopwords:\n",
    "        freq[w] = freq.get(w,0) + 1\n",
    "\n",
    "for f in sortedByValue( freq , ascending = False ) :\n",
    "    print( f'{f} -> {freq[f]}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below can be used to visualise a given dictionary with a word frequencies into a word cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud \n",
    "\n",
    "wordcloud = WordCloud( background_color=\"white\",  width=1500,height=1000, max_words= 100,relative_scaling=1,normalize_plurals=False).generate_from_frequencies(freq)\n",
    "\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the code above produced an error message, this may be caused by the fact that `wordcloud` module has not been inestalled yet on your computer. In that case, you may need to install the `wordcloud` module using the commands in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code that was explained in this notebook tokenises texts using the `word_tokenize()` method from `nltk`. As was explained, this method treats all the punction marks as separate tokens. Arguably, such occurrences of semi-colons, dots, commas and quotes should be ignored when we calculate the frequencies of words. We can do this by using a method which removes the tokens that don't contain any alphanumerical characters. \n",
    "\n",
    "In the case of tokens containing the genitival 's' (e.g. as in \"the people's choice\" or \"the child's parents\", `nltk` aims to separate the final 's' from the rest of the word. As a result of this, the character 's' will likewise be viewed and processed as a separate token. \n",
    "\n",
    "Something siliar happens with the various parts of contracted verb forms. `nltk` aims to separate the stem from the rest of token in forms such as \"don't\", \"i'm\" and \"weren't\", but this results in tokens such as \"'nt\", \"'m\" and \"'t\", respectively.  \n",
    "\n",
    "The question whether words in the genitival forms and contracted verb forms must be counted separately is of course open to debate. It can be useful to know, however, that such 'incomplete' tokens are also on `nltk`'s list of stopwords. When you filter the list of tokens using these stopwords, you will also get rid of seemingly nonsensical tokens such as \"'m\", \"'s\" and \"'t\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokeniser functions in the `tdmh` package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tdmh` package contains a function named `tokenise()`, which combines the `word_tokenize()` method from `nltk` and the `remove_puncuation()` function, discussed in the notebook on tokenisation. \n",
    "\n",
    "The package also has a function names `tokenise_remove_stopwords()` which, as is indiciated by the name, removes the stopwords as well. you can find the definition below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_remove_stopwords(full_text):\n",
    "    words = word_tokenize(full_text)\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        w = w.lower().strip()\n",
    "        orig = ''\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Counter object\n",
    "\n",
    "Python's `Counter` class, from the `collections` package, provides a very efficient way of generating frequency list from a list of values. In this notebook, we worked with a dictionary to count such items. The keys of these dictionaries contain the words to count, and the dictionary values are used to store the number of times these words are repeated. Such counts can similarly be produced via a `Counter`.\n",
    "\n",
    "As a first step, you need to import the `Counter` class from the `collections` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Counter` object can be initiated with a list of words. As was discussed above, such a list of words can be created using the `word_tokenize` method from `nltk`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "quote = '''It was the best of times, it was the worst of times, \n",
    "it was the age of wisdom, it was the age of foolishness, \n",
    "it was the epoch of belief, it was the epoch of incredulity'''\n",
    "\n",
    "words = word_tokenize(quote.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have such a list of words, it can be used as a parameter as you initialise the `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, you can also add more words to the `Counter` object using the `update()` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the result is assigned to a variable named `freq`. This variable is a specific type of dictionary. Like a regular dictionary, it consists of keys and values. \n",
    "\n",
    "The `Counter` object has a method named `most_common` which can be used to list the items witjh the highest frequencies. The parameter of this method specifies the number of items to be shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freq.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.1.\n",
    "\n",
    "Answer the following questions about this text file named \"PrideAndPrejudice.txt\", which you can find in the folder named 'Corpus'. \n",
    "\n",
    "* How many characters are there in the novel?\n",
    "* How many words does the novel contain?\n",
    "* What are the 25 most frequent words if we exclude stop words?\n",
    "* How many times does the novel contain the token 'Darcy'?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "novel = open('Corpus/PrideAndPrejudice.txt' , encoding = 'utf-8')\n",
    "full_text = novel.read()\n",
    "\n",
    "words = word_tokenize(full_text.lower())\n",
    "new_list = []\n",
    "for w in words:\n",
    "    if w.isalnum() and w not in stopwords:\n",
    "        new_list.append( w )\n",
    "\n",
    "\n",
    "freq = Counter(new_list)\n",
    "\n",
    "print( freq.most_common(50) )\n",
    "\n",
    "print(freq['darcy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3.2.\n",
    "\n",
    "Can you create a line chart that visualised the frequencies of the 25 most common words in *A Room with a View*? For this exercise, you can reuse some of the results of exercise 3.1.  \n",
    "\n",
    "To get hold of the values to be shown on the x-axis and the y-axis, you can use the `keys()` and the `values()` methods that are available for dictionaries. These methods result in specific types of objects which are called  `dict_keys` and `dict_values`. It is not easy to select the first 25 items form such object. To do this, however you can convert these objects to lists, using `list()`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
