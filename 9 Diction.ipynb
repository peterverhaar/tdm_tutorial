{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Diction\n",
    "\n",
    "The term 'diction' generally refers to the stylistic choices that are made by an author while writing a text. A study of the diction of an author may concentrate, among other things, on the words that are chosen. In stylometric research, it can be interesting to study the words that are characteristic of a given author, and to examine how the words that are chosen by one author differ from the words chosen by other authors. \n",
    "\n",
    "This notebook explains how you can corpare the words in two different text, and how you can identify the words that are distinctive within each of these texts.\n",
    "\n",
    "## Defining 'subcorpora'\n",
    "\n",
    "This notebook explains how you can corpare the words in two 'subcorpora'. Within the overarching corpus, you may have collected texts by two different authors or texts in two different genres, or texts from different periods. The code below firstly enables you to define such subcorpora. In the lists that are  named `corpus1` and `corpus2`, you need to list all the texts from the two subcorpora whoe words you want to compare. \n",
    "\n",
    "The code in this notebook carries out a comparative analysis of the diction in two novels only: *Through the Looking Glass and *Ulysses*. The former test is added as an item to `corpus1`, and the second text is appended to `corpus2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "\n",
    "corpus1 = [ 'ThroughtheLookingGlass.txt' ]\n",
    "corpus2 = [ 'HeartofDarkness.txt' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating frequencies\n",
    "\n",
    "The code in the cell below reads in the full text of the files that are listed in `corpus1`. In this case, we are dealing with one text file only. Next, we calculate the frequencies of all of these words. These frequencies are stored in a dictionary named `freq1`.\n",
    "\n",
    "Once the first subcorpus has been processed, the code does the same for the texts in `corpus2`. The word frequencies are placed in a dictionary named `freq2`.\n",
    "\n",
    "After running this code, the variable `full_text1` will contain the *complete* texts of all the texts in `corpus1`. The dictionary named `freq1` will contain the frequencies of all the words in this full text. \n",
    "\n",
    "The variables `full_text2` and `freq2` store the same type of information for the texts in `corpus2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmh import *\n",
    "from os.path import join\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenise_remove_stopwords(full_text):\n",
    "    words = word_tokenize(full_text)\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        w = w.lower().strip()\n",
    "        orig = ''\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "\n",
    "freq1 = dict()\n",
    "full_text1 = ''\n",
    "\n",
    "for text in corpus1:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text1 += file_handler.read() + ' '\n",
    "\n",
    "words = tokenise_remove_stopwords( full_text1  )\n",
    "\n",
    "for w in words:\n",
    "    freq1[w] = freq1.get(w,0) +1\n",
    "    \n",
    "        \n",
    "        \n",
    "freq2 = dict()\n",
    "full_text2 = ''\n",
    "    \n",
    "for text in corpus2:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text2 += file_handler.read() + ' '\n",
    "\n",
    "words = tokenise_remove_stopwords(  full_text2 )\n",
    "\n",
    "for w in words:\n",
    "    freq2[w] = freq2.get(w,0) +1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Dunning's log likelihood\n",
    "\n",
    "One of statistical methods that can be used to find such distinctive words is *Dunning's log likelihood*. In short, it analyses the distinctiveness of word in one set of texts compared to the texts in a reference corpus, by calculating probabilities based on word frequencies. A good explanation of the fomula can be found on the [wordHoard](https://wordhoard.northwestern.edu/userman/analysis-comparewords.html#loglike) website. \n",
    "\n",
    "Using the frequencies that have been calculated above, the Dunning log likelihood scores are calculated for all of the words that occur both in `corpus1` and `corpus2` in the cell below. The actual calculation takes place in a method named `log_likelihood()`. The scores that are calculated are all stored in a dictionary named `ll_scores`\n",
    "\n",
    "The formula that is implemented in the `log_likelihood` method returns a number which can either be positive or negative. A postive score indicates that there is a high probability that the word will be used in the first corpus. A negative probability indicates that occurence of the word is more common in the second corpus. The tokens that are assigned the highest scores, in other words, are also most distincive of the first corpus. \n",
    "\n",
    "The code below lists the 25 words that are assigned a positive log likelihood score in the first corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_likelihood( word_count1 , word_count2, total1 , total_2 ):\n",
    "\n",
    "    a = word_count1\n",
    "    b = word_count2\n",
    "    c = total1\n",
    "    d = total2\n",
    " \n",
    "    perc1 = (a/c)*100\n",
    "    perc2 = (b/d)*100\n",
    "    polarity = perc1 - perc2\n",
    " \n",
    "    E1 = c*(a+b)/(c+d)\n",
    "    E2 = d*(a+b)/(c+d)\n",
    "    \n",
    "    ln1 = math.log(a/E1)\n",
    "    ln2 = math.log(b/E2)\n",
    "    G2 = 2*((a* ln1) + (b* ln2))\n",
    "    \n",
    "    #if polarity < 0:\n",
    "    #    G2 = -G2\n",
    "    if a * math.log(a / E1) < 0:\n",
    "        G2 = -G2\n",
    "\n",
    "    return G2\n",
    "\n",
    "\n",
    "\n",
    "ll_scores = dict()\n",
    "\n",
    "total1 = 0\n",
    "total2 = 0\n",
    "\n",
    "for word1 in freq1:\n",
    "    total1 += freq1[word1]\n",
    "for word2 in freq2:\n",
    "    total2 += freq2[word2]\n",
    "\n",
    "for word in freq1:\n",
    "    if word in freq2:\n",
    "\n",
    "        ll_score = log_likelihood( freq1[word] , freq2[word] , total1 , total2 )\n",
    "        ll_scores[word] = ll_score\n",
    "\n",
    "max = 25\n",
    "i = 0 \n",
    "        \n",
    "for word in sortedByValue(ll_scores , ascending = False ):\n",
    "    print( word , ll_scores[word] )\n",
    "    i += 1\n",
    "    if i == max: \n",
    "        break        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with negative log likelihood scores are more likely to appear in the reference corpus (i.e. the second corpus) than in the first corpus. \n",
    "\n",
    "The code below lists the 25 words with the highest negative scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 25\n",
    "i = 0 \n",
    "\n",
    "for word in sortedByValue(ll_scores ) :\n",
    "    print( word , ll_scores[word] )\n",
    "    i += 1\n",
    "    if i == max:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann Whitney formula\n",
    "\n",
    "In a [blogpost on identifying literary diction](https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/), Ted Underwood argues that Dunning's log likelihood function also has a number of disadvantages. It is sensitive to outliers, for example. He explains that the Mann Whitney ranks test can be a good alternative. \n",
    "\n",
    "To perform the Mann-Whitney ranks test, we firstly need to find all the words the two corpora to be compared have in common. Next, we need to divide the full texts of the two corpora to be compared into smaller chuncks, all of the same size. These can be fragments of 500 words, for instance. Next, we need to count the number of times each word occurs in these chunks. Using these counts, we can determine whether the word is more frequent in corpus 1 than in corpus 2 (or vice versa). As a final step, we determine the total number of fragments in which the word is most frequent, both in the first and the second corpus. If it is found, using these steps, that a word is much more common in one of the two corpora, this word can be identified as a distinctive word. The Mann-Whitney ranks test really looks at occurrences across the whole corpus, and it is neutralises the effect of exceptionally high counts in one or two of these chunks.      \n",
    "\n",
    "The Mann Whitney test can be performed in Python using the `mannwhitneyu()` method from the `scipy.stats` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "## make a list of all the words in both corpora\n",
    "words1 = tokenise_remove_stopwords(full_text1)\n",
    "words2 = tokenise_remove_stopwords(full_text2)\n",
    "\n",
    "def divide_into_chunks(words, length):\n",
    "\n",
    "    chunks=[]\n",
    "    ## chunk contains dictionaries\n",
    "    # with word frequencies\n",
    "    \n",
    "    for i in range(0, len(words), length):\n",
    "        counts = dict()\n",
    "        for j in range(length):\n",
    "            if i+j < len(words):\n",
    "                word = words[i+j]\n",
    "                counts[word] = counts.get(word,0)+1\n",
    "        chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "length = 500\n",
    "chunks1 = divide_into_chunks(words1,length)\n",
    "chunks2 = divide_into_chunks(words2,length)\n",
    "\n",
    "\n",
    "# vocab is the union of terms in both sets\n",
    "all_words = dict()\n",
    "    \n",
    "for chunk in chunks1:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "for chunk in chunks2:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "    \n",
    "rho =  dict()\n",
    "    \n",
    "for word in all_words:\n",
    "        \n",
    "    a=[]\n",
    "    b=[]\n",
    "        \n",
    "    for chunk in chunks1:\n",
    "        a.append(chunk.get(word,0))\n",
    "    for chunk in chunks2:\n",
    "        b.append(chunk.get(word,0))\n",
    "\n",
    "    stat,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "    mean =len(chunks1)*len(chunks2)*0.5\n",
    "    if stat <= mean:\n",
    "        pval = 0 - pval\n",
    "            \n",
    "    rho[word]= ( pval )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are most distinctive in corpus 1 have a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The following words are most distinctive in corpus 1:\" )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho ):\n",
    "    if rho[word] > 0:\n",
    "        print( f'{word}\\t{rho[word]:.22f}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are most distinctive in corpus 2 have a negative value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The following words are most distinctive in corpus 2:\"  )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho , ascending = False ) :\n",
    "    if rho[word] < 0:\n",
    "        print( f'{word}: {rho[word]:.22f}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* Dunning, Ted, 'Accurate Methods for the Statistics of Surprise and Coincidence', in *Computational Linguistics*, 19:1 (1993).\n",
    "* Rayson, P. and Garside, R., 'Comparing corpora using frequency profiling', in *Proceedings of the workshop on Comparing Corpora, held in conjunction with the 38th annual meeting of the Association for Computational Linguistics (ACL 2000)* (2000)\n",
    "* H. Mann and D. Whitney, 'On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other', in *Ann. Math. Statist.*, 1:18 (1947). <https://doi.org/10.1214/aoms/1177730491>\n",
    "* Adam Kilgarriff, *Comparing Corpora*, in *International Journal of Corpus Linguistics*, 6:1 (2001). <https://doi.org/10.1075/ijcl.6.1.05kil>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 9.1\n",
    "\n",
    "Can you compare the diction of *Pride and Prejudice* using the Mann Whitney formula?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "\n",
    "corpus1 = [ 'PrideandPrejudice.txt' ]\n",
    "corpus2 = [ 'Ulysses.txt' ]\n",
    "\n",
    "\n",
    "def tokenise_remove_stopwords(full_text):\n",
    "    words = word_tokenize(full_text)\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        w = w.lower().strip()\n",
    "        orig = ''\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "\n",
    "full_text1 = ''\n",
    "full_text2 = ''\n",
    "\n",
    "for text in corpus1:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text1 += file_handler.read() + ' '\n",
    "\n",
    "for text in corpus2:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text2 += file_handler.read() + ' '\n",
    "\n",
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "## make a list of all the words in both corpora\n",
    "words1 = tokenise_remove_stopwords(full_text1)\n",
    "words2 = tokenise_remove_stopwords(full_text2)\n",
    "\n",
    "def divide_into_chunks(words, length):\n",
    "\n",
    "    chunks=[]\n",
    "    ## chunk contains dictionaries\n",
    "    # with word frequencies\n",
    "    \n",
    "    for i in range(0, len(words), length):\n",
    "        counts = dict()\n",
    "        for j in range(length):\n",
    "            if i+j < len(words):\n",
    "                word = words[i+j]\n",
    "                counts[word] = counts.get(word,0)+1\n",
    "        chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "length = 500\n",
    "chunks1 = divide_into_chunks(words1,length)\n",
    "chunks2 = divide_into_chunks(words2,length)\n",
    "\n",
    "\n",
    "# vocab is the union of terms in both sets\n",
    "all_words = dict()\n",
    "    \n",
    "for chunk in chunks1:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "for chunk in chunks2:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "    \n",
    "rho =  dict()\n",
    "    \n",
    "for word in all_words:\n",
    "        \n",
    "    a=[]\n",
    "    b=[]\n",
    "        \n",
    "    for chunk in chunks1:\n",
    "        a.append(chunk.get(word,0))\n",
    "    for chunk in chunks2:\n",
    "        b.append(chunk.get(word,0))\n",
    "\n",
    "    stat,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "    mean =len(chunks1)*len(chunks2)*0.5\n",
    "    if stat <= mean:\n",
    "        pval = 0 - pval\n",
    "            \n",
    "    rho[word]= ( pval )\n",
    "    \n",
    "print( f\"\\nThe following words are most distinctive in {corpus1}\" )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho ):\n",
    "    if rho[word] > 0:\n",
    "        print( f'{word}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break\n",
    "            \n",
    "\n",
    "print( f\"\\nThe following words are most distinctive in {corpus2}\" )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho , ascending = False ) :\n",
    "    if rho[word] < 0:\n",
    "        print( f'{word}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
