{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Diction\n",
    "\n",
    "The term 'diction' generally refers to the stylistic choices that are made by an author while writing a text. A study of the diction of an author may concentrate, among other things, on the words that are chosen. In stylometric research, it can be interesting to study the words that are characteristic of a given author, and to examine how the words that are chosen by one author differ from the words chosen by other authors. \n",
    "\n",
    "This notebook explains how you can corpare the words in two different texts, and how you can identify the words that are unique or distinctive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Term Document Matrix\n",
    "\n",
    "In studies of the diction in a corpus, it is generally very helpful to make a term-document matrix as a first step. A Term Document Matrix represents documents as tables in which the rows represent the words that occur in the full corpus, and in which the columns correspond to the individual texts in this corpus. The values in this matrix describe the frequencies of the words in the various texts. \n",
    "\n",
    "To make such a Term Document Matrix, we can follow the steps below. First, we make a list of all the files in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import tdmh\n",
    "import pandas as pd\n",
    "\n",
    "dir = 'Corpus'\n",
    "corpus = []\n",
    "\n",
    "for file in sorted(os.listdir(dir)):\n",
    "    if not( re.search( '^\\.' , file) ):\n",
    "        corpus.append(os.path.join(dir,file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second step, we examine the words the are used in all of these texts. The 'types' (i.e. the unique words that are found in these texts) are saved in a list named `vocabulary`. The code below also makles a dictionary named 'all_frequences' which captures the frequencies of these types in the various texts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title(book):\n",
    "    title = os.path.basename(book)\n",
    "    title = re.sub('[.]txt$', '' , title)\n",
    "    return title\n",
    "\n",
    "\n",
    "all_frequencies = dict()\n",
    "all_words = Counter()\n",
    "all_lengths = dict()\n",
    "titles = []\n",
    "\n",
    "for book in corpus:\n",
    "    print(book)\n",
    "    row = []\n",
    "    \n",
    "    ## Extract the title from the filename\n",
    "    title = extract_title(book)\n",
    "    titles.append(title)\n",
    "    \n",
    "    ## Read the full text\n",
    "    file = open(book,encoding='utf-8')\n",
    "    full_text = file.read()\n",
    "    \n",
    "    ## Find all the words in the text\n",
    "    words = tdmh.tokenise_remove_stopwords(full_text.lower())\n",
    "    nr_tokens = len(words)\n",
    "    all_lengths[title] = nr_tokens\n",
    "    \n",
    "    ## Count all the words in the text\n",
    "    freq_book = Counter(words)\n",
    "    all_frequencies[title] = freq_book\n",
    "\n",
    "    ## Update the Counter object named 'all_words'\n",
    "    all_words.update(words)\n",
    "        \n",
    "print('Done!')\n",
    "\n",
    "vocabulary = list(all_words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, based on the vocabulary and these frequency lists, we can create a term Document Matrix. Each type in the variable `vocabulary` will become a separate row in this matrix. The columns describe the frequencies of these types in the texts in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm = pd.DataFrame( columns= all_words )\n",
    "        \n",
    "for book in titles:\n",
    "    row = dict()\n",
    "    freq = all_frequencies[book]\n",
    "    for w in vocabulary:\n",
    "        row[w] = freq.get(w,0)/all_lengths[book]\n",
    "    dtm = dtm.append( row  , ignore_index=True )\n",
    "    \n",
    "tdm = dtm.transpose()\n",
    "tdm.columns = titles\n",
    "\n",
    "tdm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Term Document Matrix contain relative frequencies. In each cell, the absolute counts of the types are divided by the total number of tokens in each cell. This form of normalisation was applied to mitigate the impact of the differences in length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unique words\n",
    "\n",
    "With such a Term Document Matrix in place, it becomes much easier to identify the words that are unique to a given text. To find the words that are unique to *A Room with a View*, for example, we need to find those words from the novel which are *not* used in any of the other texts in the corpus. In other words, the frequencies of these words need to be zero in all of the other texts. The method `find_unique_words()` in the code below implements a method that can be used to identfy these unique words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_words(book,nr_words=50):\n",
    "    unique = tdm.copy()\n",
    "    columns_other = list(unique.columns)\n",
    "    columns_other.remove(book)\n",
    "\n",
    "    unique['sum_other'] = unique[columns_other].sum(axis=1) \n",
    "    unique = unique.query( 'sum_other == 0' )\n",
    "    unique = unique.query( f'{book} > 0' )\n",
    "    unique = unique.sort_values(book,ascending=False)[book][:nr_words]\n",
    "    return list(unique.keys())\n",
    "\n",
    "title = 'ARoomWithaView'\n",
    "unique_words = find_unique_words(title)\n",
    "print(f'{title}\\n{unique_words}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below loops through all the texts in the corpus and identifies, for each text, the most frequent unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in corpus:\n",
    "    unique_words = find_unique_words( extract_title(text) )\n",
    "    print(f'{text}\\n{unique_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distinctive words\n",
    "\n",
    "Next to establishing the **unique** words, it can also be interesting to identify those words which are very frequently in one text, in comparison to the other texts in the corpus. \n",
    "\n",
    "To identify such distinctive words, we firstly calculate the average frequency of each word in the corpus. This average will function as the 'norm' against which we can measure the various frequencies. To do this, we can make use of the `mean()` method in `pandas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm['average'] = tdm.mean(numeric_only=True,axis=1)\n",
    "tdm['standard_deviation'] = tdm.std(numeric_only=True,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know the average values, we can also assess the differences between the frequencies in individual texts on the one hand and the average for the corpus on the other. \n",
    "\n",
    "The cell below adds new columns with the suffix '_distinction' for each text in the corpus. The 'distinction' is calculated by subtracting the average frequency from the frequency that was found in that individual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for book in titles:\n",
    "    print(book)\n",
    "    column_name = f'{book}_distinction'\n",
    "    values = []\n",
    "    \n",
    "    for word,freq in tdm.iterrows():\n",
    "\n",
    "        score = (freq[book] - freq['average'])\n",
    "        if pd.isna(score):\n",
    "            score = 0\n",
    "        values.append(score)\n",
    "    tdm[column_name] = values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_distinctive_words(df,book): \n",
    "    column_name = f'{book}_distinction'\n",
    "    return df[column_name].sort_values( ascending = False )[:30]\n",
    "\n",
    "for title in titles:\n",
    "    print(f'\\n{title.upper()}\\n')\n",
    "    for word,count in most_distinctive_words(tdm,title).items():\n",
    "        print( f'{word} ({count}) ' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining 'subcorpora'\n",
    "\n",
    "It may also be the case that you have different 'subcorpora' within your overarching corpus. You may have collected texts by two different authors, texts in two different genres, or texts from different periods. It can be interesting to examine the words that are distinctive for such clusters of texts. \n",
    "\n",
    "The code below firstly enables you to define such subcorpora. In the lists that are  named `corpus1` and `corpus2`, you need to list all the texts from the two subcorpora whoe words you want to compare. \n",
    "\n",
    "The code in this notebook carries out a comparative analysis of the diction in the novels form the Ninetheenth and the Twentieth century. The former texts are added as an item to `corpus1`, and the second texts are appended to `corpus2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = 'Corpus'\n",
    "\n",
    "corpus1 = [ 'PrideandPrejudice.txt','ThroughtheLookingGlass.txt','ATaleofTwoCities.txt' ]\n",
    "corpus2 = [ 'HeartofDarkness.txt','ARoomWithaView.txt','Ulysses.txt']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating frequencies\n",
    "\n",
    "The code in the cell below reads in the full text of the files that are listed in `corpus1`. In this case, we are dealing with one text file only. Next, we calculate the frequencies of all of these words. These frequencies are stored in a dictionary named `freq1`.\n",
    "\n",
    "Once the first subcorpus has been processed, the code does the same for the texts in `corpus2`. The word frequencies are placed in a dictionary named `freq2`.\n",
    "\n",
    "After running this code, the variable `full_text1` will contain the *complete* texts of all the texts in `corpus1`. The dictionary named `freq1` will contain the frequencies of all the words in this full text. \n",
    "\n",
    "The variables `full_text2` and `freq2` store the same type of information for the texts in `corpus2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdmh import *\n",
    "from os.path import join\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def tokenise_remove_stopwords(full_text):\n",
    "    words = word_tokenize(full_text)\n",
    "    new_list= []\n",
    "    for w in words:\n",
    "        w = w.lower().strip()\n",
    "        orig = ''\n",
    "        if w.isalnum() and w not in stopwords:\n",
    "            new_list.append( w )\n",
    "    return new_list\n",
    "\n",
    "\n",
    "freq1 = dict()\n",
    "full_text1 = ''\n",
    "\n",
    "for text in corpus1:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text1 += file_handler.read() + ' '\n",
    "\n",
    "words = tokenise_remove_stopwords( full_text1  )\n",
    "\n",
    "for w in words:\n",
    "    freq1[w] = freq1.get(w,0) +1\n",
    "    \n",
    "        \n",
    "        \n",
    "freq2 = dict()\n",
    "full_text2 = ''\n",
    "    \n",
    "for text in corpus2:\n",
    "    print('Reading ' + text + ' ... ')\n",
    "    with open( join( dir,text) ) as file_handler:\n",
    "        full_text2 += file_handler.read() + ' '\n",
    "\n",
    "words = tokenise_remove_stopwords(  full_text2 )\n",
    "\n",
    "for w in words:\n",
    "    freq2[w] = freq2.get(w,0) +1\n",
    "    \n",
    "print('Done!')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Dunning's log likelihood\n",
    "\n",
    "One of statistical methods that can be used to find such distinctive words is *Dunning's log likelihood*. In short, it analyses the distinctiveness of word in one set of texts compared to the texts in a reference corpus, by calculating probabilities based on word frequencies. A good explanation of the fomula can be found on the [wordHoard](https://wordhoard.northwestern.edu/userman/analysis-comparewords.html#loglike) website. \n",
    "\n",
    "Using the frequencies that have been calculated above, the Dunning log likelihood scores are calculated for all of the words that occur both in `corpus1` and `corpus2` in the cell below. The actual calculation takes place in a method named `log_likelihood()`. The scores that are calculated are all stored in a dictionary named `ll_scores`\n",
    "\n",
    "The formula that is implemented in the `log_likelihood` method returns a number which can either be positive or negative. A postive score indicates that there is a high probability that the word will be used in the first corpus. A negative probability indicates that occurence of the word is more common in the second corpus. The tokens that are assigned the highest scores, in other words, are also most distincive of the first corpus. \n",
    "\n",
    "The code below lists the 25 words that are assigned a positive log likelihood score in the first corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def log_likelihood( word_count1 , word_count2, total1 , total_2 ):\n",
    "\n",
    "    a = word_count1\n",
    "    b = word_count2\n",
    "    c = total1\n",
    "    d = total2\n",
    " \n",
    "    perc1 = (a/c)*100\n",
    "    perc2 = (b/d)*100\n",
    "    polarity = perc1 - perc2\n",
    " \n",
    "    E1 = c*(a+b)/(c+d)\n",
    "    E2 = d*(a+b)/(c+d)\n",
    "    \n",
    "    ln1 = math.log(a/E1)\n",
    "    ln2 = math.log(b/E2)\n",
    "    G2 = 2*((a* ln1) + (b* ln2))\n",
    "    \n",
    "    #if polarity < 0:\n",
    "    #    G2 = -G2\n",
    "    if a * math.log(a / E1) < 0:\n",
    "        G2 = -G2\n",
    "\n",
    "    return G2\n",
    "\n",
    "\n",
    "\n",
    "ll_scores = dict()\n",
    "\n",
    "total1 = 0\n",
    "total2 = 0\n",
    "\n",
    "for word1 in freq1:\n",
    "    total1 += freq1[word1]\n",
    "for word2 in freq2:\n",
    "    total2 += freq2[word2]\n",
    "\n",
    "for word in freq1:\n",
    "    if word in freq2:\n",
    "\n",
    "        ll_score = log_likelihood( freq1[word] , freq2[word] , total1 , total2 )\n",
    "        ll_scores[word] = ll_score\n",
    "\n",
    "max = 25\n",
    "i = 0 \n",
    "        \n",
    "for word in sortedByValue(ll_scores , ascending = False ):\n",
    "    print( word , ll_scores[word] )\n",
    "    i += 1\n",
    "    if i == max: \n",
    "        break        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words with negative log likelihood scores are more likely to appear in the reference corpus (i.e. the second corpus) than in the first corpus. \n",
    "\n",
    "The code below lists the 25 words with the highest negative scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max = 25\n",
    "i = 0 \n",
    "\n",
    "for word in sortedByValue(ll_scores ) :\n",
    "    print( word , ll_scores[word] )\n",
    "    i += 1\n",
    "    if i == max:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mann Whitney formula\n",
    "\n",
    "In a [blogpost on identifying literary diction](https://tedunderwood.com/2011/11/09/identifying-the-terms-that-characterize-an-author-or-genre-why-dunnings-may-not-be-the-best-method/), Ted Underwood argues that Dunning's log likelihood function also has a number of disadvantages. It is sensitive to outliers, for example. He explains that the Mann Whitney ranks test can be a good alternative. \n",
    "\n",
    "To perform the Mann-Whitney ranks test, we firstly need to find all the words the two corpora to be compared have in common. Next, we need to divide the full texts of the two corpora to be compared into smaller chuncks, all of the same size. These can be fragments of 500 words, for instance. Next, we need to count the number of times each word occurs in these chunks. Using these counts, we can determine whether the word is more frequent in corpus 1 than in corpus 2 (or vice versa). As a final step, we determine the total number of fragments in which the word is most frequent, both in the first and the second corpus. If it is found, using these steps, that a word is much more common in one of the two corpora, this word can be identified as a distinctive word. The Mann-Whitney ranks test really looks at occurrences across the whole corpus, and it is neutralises the effect of exceptionally high counts in one or two of these chunks.      \n",
    "\n",
    "The Mann Whitney test can be performed in Python using the `mannwhitneyu()` method from the `scipy.stats` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "## make a list of all the words in both corpora\n",
    "words1 = tokenise_remove_stopwords(full_text1)\n",
    "words2 = tokenise_remove_stopwords(full_text2)\n",
    "\n",
    "def divide_into_chunks(words, length):\n",
    "\n",
    "    chunks=[]\n",
    "    ## chunk contains dictionaries\n",
    "    # with word frequencies\n",
    "    \n",
    "    for i in range(0, len(words), length):\n",
    "        counts = dict()\n",
    "        for j in range(length):\n",
    "            if i+j < len(words):\n",
    "                word = words[i+j]\n",
    "                counts[word] = counts.get(word,0)+1\n",
    "        chunks.append(counts)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "length = 500\n",
    "chunks1 = divide_into_chunks(words1,length)\n",
    "chunks2 = divide_into_chunks(words2,length)\n",
    "\n",
    "\n",
    "# vocab is the union of terms in both sets\n",
    "all_words = dict()\n",
    "    \n",
    "for chunk in chunks1:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "for chunk in chunks2:\n",
    "    for word in chunk:\n",
    "        all_words[word]= all_words.get(word,0) + 1\n",
    "    \n",
    "rho =  dict()\n",
    "    \n",
    "for word in all_words:\n",
    "        \n",
    "    a=[]\n",
    "    b=[]\n",
    "        \n",
    "    for chunk in chunks1:\n",
    "        a.append(chunk.get(word,0))\n",
    "    for chunk in chunks2:\n",
    "        b.append(chunk.get(word,0))\n",
    "\n",
    "    stat,pval=mannwhitneyu(a,b, alternative=\"two-sided\")\n",
    "    mean =len(chunks1)*len(chunks2)*0.5\n",
    "    if stat <= mean:\n",
    "        pval = 0 - pval\n",
    "            \n",
    "    rho[word]= ( pval )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are most distinctive in corpus 1 have a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The following words are most distinctive in corpus 1:\" )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho ):\n",
    "    if rho[word] > 0:\n",
    "        print( f'{word}\\t{rho[word]:.22f}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words that are most distinctive in corpus 2 have a negative value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"The following words are most distinctive in corpus 2:\"  )  \n",
    "\n",
    "i = 0\n",
    "max = 25\n",
    "\n",
    "for word in sortedByValue( rho , ascending = False ) :\n",
    "    if rho[word] < 0:\n",
    "        print( f'{word}: {rho[word]:.22f}' ) \n",
    "        i += 1\n",
    "        if i == max:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "\n",
    "* Dunning, Ted, 'Accurate Methods for the Statistics of Surprise and Coincidence', in *Computational Linguistics*, 19:1 (1993).\n",
    "* Rayson, P. and Garside, R., 'Comparing corpora using frequency profiling', in *Proceedings of the workshop on Comparing Corpora, held in conjunction with the 38th annual meeting of the Association for Computational Linguistics (ACL 2000)* (2000)\n",
    "* H. Mann and D. Whitney, 'On a Test of Whether one of Two Random Variables is Stochastically Larger than the Other', in *Ann. Math. Statist.*, 1:18 (1947). <https://doi.org/10.1214/aoms/1177730491>\n",
    "* Adam Kilgarriff, *Comparing Corpora*, in *International Journal of Corpus Linguistics*, 6:1 (2001). <https://doi.org/10.1075/ijcl.6.1.05kil>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
