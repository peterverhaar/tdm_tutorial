{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for other languages\n",
    "\n",
    "Many of the methods in the `NLTK` library, such as `pos_tag`, were trained on texts in modern English. If you want to work with other languages, you need to change the model underlying these methods. \n",
    "\n",
    "For texts in the Dutch langauge, for instance, you can make use of the model trained on the [Alpino](https://www.let.rug.nl/~vannoord/trees/) corpus. You can change the NLTK language model as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('alpino')\n",
    "\n",
    "from nltk.corpus import alpino as alp\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "training_corpus = alp.tagged_sents()\n",
    "unitagger = UnigramTagger(training_corpus)\n",
    "bitagger = BigramTagger(training_corpus, backoff=unitagger)\n",
    "pos_tag = bitagger.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdm import word_tokenise\n",
    "\n",
    "sentence = 'Het was nog donker, toen in de vroege morgen van de tweeÃ«ntwintigste december 1946 in onze stad, op de eerste verdieping van het huis Schilderskade 66, de held van deze geschiedenis, Frits van Egters, ontwaakte.'\n",
    "\n",
    "words = word_tokenise(sentence)\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to make use of another NLP library named [spaCy](https://spacy.io/). This NLP library offers support for [a wide range of languages](https://spacy.io/usage/models). These langauge models can all be downloaded from the spaCy website. \n",
    "\n",
    "spaCy is not part of the Anaconda distribution of Python, so if you have never worked with spaCy before, the library needs to be installed first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!pip install spacy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of [language models](https://spacy.io/models/nl). You can use the code below to download the model named `nl_core_news_lg`. This is a langauge model for Dutch. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!python3 -m spacy download nl_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!python3 -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has been downloaded, it needs to be loaded into your code, so that you can start to work with it. The `load()` method in `spaCy` creates a new object which can be used to add linguistic and semantic annotations. in the cell below, this object is given the name `nlp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"nl_core_news_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This newly created `nlp` object can given a string as input. Its output will be a tagged text giving information about a number of grammatical and morphological aspects of this string, including the parts of speech, the sentence boundaries and the lemmatised form. \n",
    "\n",
    "In the code below, the output of the `nlp()` method is assigned to a variable named `tagged_text`. The annotations can be accessed by naviagting through the string word by word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nlp.get_pipe(\"lemmatizer\")\n",
    "tagged_text = nlp(\"'Het is gezien', mompelde hij, 'het is niet onopgemerkt gebleven.''\")\n",
    "\n",
    "for w in tagged_text:\n",
    "    print( f'{w.text} (pos: {w.pos_} ; lemma: {w.lemma_})' )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below aims to use `spaCy` to produce data about the number of words, sentences, adverbs, pronouns, adjectives and conjunctions for all the texts in a folder named 'Corpus'. The process of adding linguistic annotations may demand some time, unfortunately. The code below used the `timeit` library to track how long this process actually takes. With longer texts, this process may take more than a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "from tdm import removeExtension\n",
    "import spacy\n",
    "import os\n",
    "import re\n",
    "\n",
    "dir = 'Corpus'\n",
    "\n",
    "out = open( 'nlp.csv' , 'w' ,  encoding = 'utf-8')\n",
    "\n",
    "# CSV header\n",
    "out.write( 'title,tokens,sentences,' )\n",
    "out.write(  'adverbs,verbs,pronouns,nouns,adjectives,conjunctions,aux-verbs\\n')\n",
    "\n",
    "\n",
    "for file in os.listdir(dir):\n",
    "    if re.search( r'.txt$' , file ):\n",
    "        print( f'Adding annotations for {file} ... ')\n",
    "        out.write( removeExtension(file) )\n",
    "        path = os.path.join(dir,text)\n",
    "        with open(path) as file_handler:\n",
    "            full_text = file_handler.read()\n",
    "        start_time = timeit.default_timer()\n",
    "        annotated_text = nlp(full_text)\n",
    "        end_time = timeit.default_timer()\n",
    "        print( f'Done! The annotation process took {end_time-start_time} seconds.')\n",
    "        nr_words = len(annotated_text) \n",
    "        nr_sentences = len(list( annotated_text.sents ))\n",
    "        out.write( f',{nr_words},{nr_sentences}')\n",
    "\n",
    "        for w in annotated_text:\n",
    "            pos[ w.pos_ ] = pos.get( w.pos_ , 0 ) + 1\n",
    "            \n",
    "        out.write( f\",{pos.get('ADV',0)}\" )\n",
    "        out.write( f\",{pos.get('VERB',0)}\" )\n",
    "        out.write( f\",{pos.get('PRON',0)}\" )\n",
    "        out.write( f\",{pos.get('NOUN',0)}\" )\n",
    "        out.write( f\",{pos.get('ADJ',0)}\" )\n",
    "        out.write( f\",{pos.get('SCONJ',0)+pos.get('CCONJ',0)}\" )\n",
    "        out.write( f\",{pos.get('AUX',0)}\" )\n",
    "        out.write( '\\n')\n",
    "            \n",
    "out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
